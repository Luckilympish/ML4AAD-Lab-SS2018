2018-05-30 23:08:36:INFO:smac.utils.io.cmd_reader.CMDReader:Output to smac3-output_2018-05-30_23:08:36_900666
2018-05-30 23:08:36:DEBUG:smac.utils.io.cmd_reader.CMDReader:Could not parse pcs file with old format; trying new format ...
2018-05-30 23:08:36:DEBUG:smac.scenario.scenario.Scenario:SMAC and Scenario Options:
2018-05-30 23:08:36:DEBUG:smac.scenario.scenario.Scenario:Output directory does not exist! Will be created.
2018-05-30 23:08:36:DEBUG:smac.scenario.scenario.Scenario:Writing scenario-file to smac3-output_2018-05-30_23:08:36_900666/run_1/scenario.txt.
2018-05-30 23:08:36:DEBUG:smac.tae.execute_ta_run_old.ExecuteTARunOld:Calling: python3 ./dn_net/KerasWrapper.py --mem-limit 3072 instances/train/instance1.csv 0 99999999999999.0 0 491263 -activation relu -batch_size 10 -epochs 10 -kernel_initializer uniform -neurons 1 -optimizer Adam
2018-05-30 23:08:40:DEBUG:smac.tae.execute_ta_run_old.ExecuteTARunOld:Stdout: [[8.00e+00 1.88e+02 7.80e+01 ... 1.37e-01 4.30e+01 1.00e+00]
 [7.00e+00 1.52e+02 8.80e+01 ... 3.37e-01 3.60e+01 1.00e+00]
 [2.00e+00 9.90e+01 5.20e+01 ... 6.37e-01 2.10e+01 0.00e+00]
 ...
 [5.00e+00 1.21e+02 7.20e+01 ... 2.45e-01 3.00e+01 0.00e+00]
 [1.00e+00 1.26e+02 6.00e+01 ... 3.49e-01 4.70e+01 1.00e+00]
 [1.00e+00 9.30e+01 7.00e+01 ... 3.15e-01 2.30e+01 0.00e+00]]
Epoch 1/10

 10/491 [..............................] - ETA: 5s - loss: 0.7427 - acc: 0.1000
440/491 [=========================>....] - ETA: 0s - loss: 0.6949 - acc: 0.6250
491/491 [==============================] - 0s 358us/step - loss: 0.6943 - acc: 0.6273
Epoch 2/10

 10/491 [..............................] - ETA: 0s - loss: 0.6869 - acc: 0.7000
440/491 [=========================>....] - ETA: 0s - loss: 0.6877 - acc: 0.6432
491/491 [==============================] - 0s 120us/step - loss: 0.6869 - acc: 0.6517
Epoch 3/10

 10/491 [..............................] - ETA: 0s - loss: 0.6875 - acc: 0.6000
440/491 [=========================>....] - ETA: 0s - loss: 0.6826 - acc: 0.6523
491/491 [==============================] - 0s 119us/step - loss: 0.6824 - acc: 0.6517
Epoch 4/10

 10/491 [..............................] - ETA: 0s - loss: 0.6563 - acc: 0.9000
440/491 [=========================>....] - ETA: 0s - loss: 0.6786 - acc: 0.6477
491/491 [==============================] - 0s 119us/step - loss: 0.6780 - acc: 0.6517
Epoch 5/10

 10/491 [..............................] - ETA: 0s - loss: 0.6951 - acc: 0.5000
440/491 [=========================>....] - ETA: 0s - loss: 0.6743 - acc: 0.6523
491/491 [==============================] - 0s 119us/step - loss: 0.6739 - acc: 0.6538
Epoch 6/10

 10/491 [..............................] - ETA: 0s - loss: 0.6650 - acc: 0.7000
440/491 [=========================>....] - ETA: 0s - loss: 0.6710 - acc: 0.6545
491/491 [==============================] - 0s 119us/step - loss: 0.6710 - acc: 0.6538
Epoch 7/10

 10/491 [..............................] - ETA: 0s - loss: 0.6429 - acc: 0.8000
440/491 [=========================>....] - ETA: 0s - loss: 0.6711 - acc: 0.6409
491/491 [==============================] - 0s 120us/step - loss: 0.6684 - acc: 0.6538
Epoch 8/10

 10/491 [..............................] - ETA: 0s - loss: 0.6169 - acc: 0.9000
440/491 [=========================>....] - ETA: 0s - loss: 0.6662 - acc: 0.6523
491/491 [==============================] - 0s 120us/step - loss: 0.6657 - acc: 0.6538
Epoch 9/10

 10/491 [..............................] - ETA: 0s - loss: 0.6767 - acc: 0.6000
440/491 [=========================>....] - ETA: 0s - loss: 0.6638 - acc: 0.6523
491/491 [==============================] - 0s 120us/step - loss: 0.6633 - acc: 0.6538
Epoch 10/10

 10/491 [..............................] - ETA: 0s - loss: 0.6758 - acc: 0.6000
440/491 [=========================>....] - ETA: 0s - loss: 0.6624 - acc: 0.6500
491/491 [==============================] - 0s 120us/step - loss: 0.6613 - acc: 0.6538
[GenericWrapper][DEBUG] Calling runsolver. Command-line:
[GenericWrapper][DEBUG] /home/shalag/.local/lib/python3.5/site-packages/genericWrapper4AC/binaries/runsolver -M 3072 -C 2147483647 -w "/tmp/383598.1.lab_course.q/watcher-731558-oi98lfk3.log" -o "/tmp/383598.1.lab_course.q/solver-731558-558z51cd.log" 
[GenericWrapper][DEBUG] Measured wallclock time: 0.009613
[GenericWrapper][DEBUG] Reading runsolver output from /tmp/383598.1.lab_course.q/watcher-731558-oi98lfk3.log
[GenericWrapper][DEBUG] Measured time by runsolver: 0.009613
Result for ParamILS: SAT, 0.0096, -1, 0.6422764227642277, 491263

2018-05-30 23:08:40:DEBUG:smac.tae.execute_ta_run_old.ExecuteTARunOld:Stderr: 2018-05-30 23:08:39.310530: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
/home/shalag/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.

2018-05-30 23:08:40:DEBUG:smac.tae.execute_ta_run_old.ExecuteTARunOld:Return: Status: <StatusType.SUCCESS: 1>, cost: 0.642276, time: 0.009600, additional: {}
2018-05-30 23:08:40:DEBUG:smac.runhistory.runhistory2epm.RunHistory2EPM4Cost:Transform runhistory into X,y format
2018-05-30 23:08:40:DEBUG:smac.runhistory.runhistory2epm.RunHistory2EPM4Cost:Converted 1 observations
2018-05-30 23:08:40:DEBUG:smac.optimizer.smbo.SMBO:Search for next configuration
2018-05-30 23:08:40:DEBUG:smac.optimizer.ei_optimization.LocalSearch:Local search took 11 steps and looked at 352 configurations. Computing the acquisition value for one configuration took 0.000539 seconds on average.
2018-05-30 23:08:40:DEBUG:smac.optimizer.ei_optimization.InterleavedLocalAndRandomSearch:First 10 acq func (origin) values of selected configurations: [[0.00126156626101008, 'Random Search (sorted)'], [0.00126156626101008, 'Random Search (sorted)'], [0.00126156626101008, 'Random Search (sorted)'], [0.00126156626101008, 'Random Search (sorted)'], [0.00126156626101008, 'Random Search (sorted)'], [0.00126156626101008, 'Random Search (sorted)'], [0.00126156626101008, 'Random Search (sorted)'], [0.00126156626101008, 'Random Search (sorted)'], [0.00126156626101008, 'Random Search (sorted)'], [0.00126156626101008, 'Random Search (sorted)']]
2018-05-30 23:08:40:DEBUG:smac.optimizer.smbo.SMBO:Total time: 0.7105, time spent on choosing next configurations: 0.3552 (0.50), time left for intensification: 0.3552 (0.50)
2018-05-30 23:08:40:DEBUG:smac.optimizer.smbo.SMBO:Intensify
2018-05-30 23:08:40:DEBUG:smac.intensification.intensification.Intensifier:Intensify on Configuration:
  activation, Value: 'hard_sigmoid'
  batch_size, Value: 26
  epochs, Value: 54
  kernel_initializer, Value: 'lecun_uniform'
  neurons, Value: 1
  optimizer, Value: 'Adagrad'

2018-05-30 23:08:40:DEBUG:smac.intensification.intensification.Intensifier:Configuration origin: Random Search (sorted)
2018-05-30 23:08:40:DEBUG:smac.intensification.intensification.Intensifier:Add run of incumbent
2018-05-30 23:08:40:DEBUG:smac.tae.execute_ta_run_old.ExecuteTARunOld:Calling: python3 ./dn_net/KerasWrapper.py --mem-limit 3072 instances/train/instance4.csv 0 99999999999999.0 0 550290313 -activation relu -batch_size 10 -epochs 10 -kernel_initializer uniform -neurons 1 -optimizer Adam
2018-05-30 23:08:43:DEBUG:smac.tae.execute_ta_run_old.ExecuteTARunOld:Stdout: [[  6.    148.     72.    ...   0.627  50.      1.   ]
 [  1.     85.     66.    ...   0.351  31.      0.   ]
 [  8.    183.     64.    ...   0.672  32.      1.   ]
 ...
 [  5.    121.     72.    ...   0.245  30.      0.   ]
 [  1.    126.     60.    ...   0.349  47.      1.   ]
 [  1.     93.     70.    ...   0.315  23.      0.   ]]
Epoch 1/10

 10/492 [..............................] - ETA: 5s - loss: 4.1008 - acc: 0.7000
440/492 [=========================>....] - ETA: 0s - loss: 1.7079 - acc: 0.4773
492/492 [==============================] - 0s 355us/step - loss: 1.6009 - acc: 0.4654
Epoch 2/10

 10/492 [..............................] - ETA: 0s - loss: 0.6893 - acc: 0.7000
440/492 [=========================>....] - ETA: 0s - loss: 0.6943 - acc: 0.3977
492/492 [==============================] - 0s 119us/step - loss: 0.6941 - acc: 0.4268
Epoch 3/10

 10/492 [..............................] - ETA: 0s - loss: 0.6918 - acc: 0.7000
440/492 [=========================>....] - ETA: 0s - loss: 0.6910 - acc: 0.6273
492/492 [==============================] - 0s 119us/step - loss: 0.6908 - acc: 0.6280
Epoch 4/10

 10/492 [..............................] - ETA: 0s - loss: 0.6832 - acc: 0.8000
440/492 [=========================>....] - ETA: 0s - loss: 0.6874 - acc: 0.6318
492/492 [==============================] - 0s 119us/step - loss: 0.6874 - acc: 0.6280
Epoch 5/10

 10/492 [..............................] - ETA: 0s - loss: 0.6747 - acc: 0.8000
440/492 [=========================>....] - ETA: 0s - loss: 0.6850 - acc: 0.6205
492/492 [==============================] - 0s 119us/step - loss: 0.6842 - acc: 0.6280
Epoch 6/10

 10/492 [..............................] - ETA: 0s - loss: 0.6941 - acc: 0.5000
440/492 [=========================>....] - ETA: 0s - loss: 0.6828 - acc: 0.6159
492/492 [==============================] - 0s 119us/step - loss: 0.6813 - acc: 0.6280
Epoch 7/10

 10/492 [..............................] - ETA: 0s - loss: 0.7062 - acc: 0.4000
440/492 [=========================>....] - ETA: 0s - loss: 0.6784 - acc: 0.6341
492/492 [==============================] - 0s 119us/step - loss: 0.6791 - acc: 0.6280
Epoch 8/10

 10/492 [..............................] - ETA: 0s - loss: 0.7096 - acc: 0.4000
440/492 [=========================>....] - ETA: 0s - loss: 0.6765 - acc: 0.6295
492/492 [==============================] - 0s 119us/step - loss: 0.6766 - acc: 0.6280
Epoch 9/10

 10/492 [..............................] - ETA: 0s - loss: 0.6477 - acc: 0.8000
440/492 [=========================>....] - ETA: 0s - loss: 0.6752 - acc: 0.6250
492/492 [==============================] - 0s 119us/step - loss: 0.6748 - acc: 0.6280
Epoch 10/10

 10/492 [..............................] - ETA: 0s - loss: 0.6975 - acc: 0.5000
440/492 [=========================>....] - ETA: 0s - loss: 0.6723 - acc: 0.6318
492/492 [==============================] - 0s 119us/step - loss: 0.6729 - acc: 0.6280
[GenericWrapper][DEBUG] Calling runsolver. Command-line:
[GenericWrapper][DEBUG] /home/shalag/.local/lib/python3.5/site-packages/genericWrapper4AC/binaries/runsolver -M 3072 -C 2147483647 -w "/tmp/383598.1.lab_course.q/watcher-328733-r0_dsmqr.log" -o "/tmp/383598.1.lab_course.q/solver-328733-5zsz1dq5.log" 
[GenericWrapper][DEBUG] Measured wallclock time: 0.009492
[GenericWrapper][DEBUG] Reading runsolver output from /tmp/383598.1.lab_course.q/watcher-328733-r0_dsmqr.log
[GenericWrapper][DEBUG] Measured time by runsolver: 0.009492
Result for ParamILS: SAT, 0.0095, -1, 0.6260162601626016, 550290313

2018-05-30 23:08:43:DEBUG:smac.tae.execute_ta_run_old.ExecuteTARunOld:Stderr: 2018-05-30 23:08:42.829352: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
/home/shalag/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
Exception ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x2b56316c1f60>>
Traceback (most recent call last):
  File "/home/shalag/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 702, in __del__
TypeError: 'NoneType' object is not callable

2018-05-30 23:08:43:DEBUG:smac.tae.execute_ta_run_old.ExecuteTARunOld:Return: Status: <StatusType.SUCCESS: 1>, cost: 0.626016, time: 0.009500, additional: {}
2018-05-30 23:08:43:DEBUG:smac.intensification.intensification.Intensifier:Add run of challenger
2018-05-30 23:08:43:DEBUG:smac.tae.execute_ta_run_old.ExecuteTARunOld:Calling: python3 ./dn_net/KerasWrapper.py --mem-limit 3072 instances/train/instance1.csv 0 99999999999999.0 0 491263 -activation hard_sigmoid -batch_size 26 -epochs 54 -kernel_initializer lecun_uniform -neurons 1 -optimizer Adagrad
2018-05-30 23:08:47:DEBUG:smac.tae.execute_ta_run_old.ExecuteTARunOld:Stdout: [[8.00e+00 1.88e+02 7.80e+01 ... 1.37e-01 4.30e+01 1.00e+00]
 [7.00e+00 1.52e+02 8.80e+01 ... 3.37e-01 3.60e+01 1.00e+00]
 [2.00e+00 9.90e+01 5.20e+01 ... 6.37e-01 2.10e+01 0.00e+00]
 ...
 [5.00e+00 1.21e+02 7.20e+01 ... 2.45e-01 3.00e+01 0.00e+00]
 [1.00e+00 1.26e+02 6.00e+01 ... 3.49e-01 4.70e+01 1.00e+00]
 [1.00e+00 9.30e+01 7.00e+01 ... 3.15e-01 2.30e+01 0.00e+00]]
Epoch 1/54

 26/491 [>.............................] - ETA: 1s - loss: 0.7455 - acc: 0.3462
491/491 [==============================] - 0s 241us/step - loss: 0.7206 - acc: 0.4725
Epoch 2/54

 26/491 [>.............................] - ETA: 0s - loss: 0.7001 - acc: 0.5385
491/491 [==============================] - 0s 45us/step - loss: 0.6942 - acc: 0.5723
Epoch 3/54

 26/491 [>.............................] - ETA: 0s - loss: 0.7045 - acc: 0.5000
491/491 [==============================] - 0s 45us/step - loss: 0.6838 - acc: 0.6212
Epoch 4/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6712 - acc: 0.6923
491/491 [==============================] - 0s 45us/step - loss: 0.6789 - acc: 0.6334
Epoch 5/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6918 - acc: 0.5385
491/491 [==============================] - 0s 45us/step - loss: 0.6764 - acc: 0.6395
Epoch 6/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6780 - acc: 0.6154
491/491 [==============================] - 0s 45us/step - loss: 0.6744 - acc: 0.6395
Epoch 7/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6588 - acc: 0.7308
491/491 [==============================] - 0s 45us/step - loss: 0.6729 - acc: 0.6395
Epoch 8/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6692 - acc: 0.6538
491/491 [==============================] - 0s 45us/step - loss: 0.6715 - acc: 0.6415
Epoch 9/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6538 - acc: 0.7308
491/491 [==============================] - 0s 45us/step - loss: 0.6704 - acc: 0.6415
Epoch 10/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6753 - acc: 0.6154
491/491 [==============================] - 0s 45us/step - loss: 0.6692 - acc: 0.6436
Epoch 11/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6449 - acc: 0.7692
491/491 [==============================] - 0s 45us/step - loss: 0.6682 - acc: 0.6436
Epoch 12/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6576 - acc: 0.6923
491/491 [==============================] - 0s 45us/step - loss: 0.6671 - acc: 0.6456
Epoch 13/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6671 - acc: 0.6538
491/491 [==============================] - 0s 45us/step - loss: 0.6662 - acc: 0.6456
Epoch 14/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6909 - acc: 0.5385
491/491 [==============================] - 0s 45us/step - loss: 0.6653 - acc: 0.6477
Epoch 15/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6451 - acc: 0.7308
491/491 [==============================] - 0s 45us/step - loss: 0.6645 - acc: 0.6477
Epoch 16/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6439 - acc: 0.7308
491/491 [==============================] - 0s 45us/step - loss: 0.6637 - acc: 0.6477
Epoch 17/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6681 - acc: 0.6154
491/491 [==============================] - 0s 45us/step - loss: 0.6630 - acc: 0.6477
Epoch 18/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6616 - acc: 0.6538
491/491 [==============================] - 0s 45us/step - loss: 0.6624 - acc: 0.6477
Epoch 19/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6302 - acc: 0.7692
491/491 [==============================] - 0s 45us/step - loss: 0.6618 - acc: 0.6477
Epoch 20/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6565 - acc: 0.6538
491/491 [==============================] - 0s 45us/step - loss: 0.6612 - acc: 0.6477
Epoch 21/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6171 - acc: 0.8077
491/491 [==============================] - 0s 45us/step - loss: 0.6607 - acc: 0.6497
Epoch 22/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6704 - acc: 0.6154
491/491 [==============================] - 0s 45us/step - loss: 0.6601 - acc: 0.6497
Epoch 23/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6702 - acc: 0.6154
491/491 [==============================] - 0s 45us/step - loss: 0.6596 - acc: 0.6497
Epoch 24/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6018 - acc: 0.8462
491/491 [==============================] - 0s 45us/step - loss: 0.6592 - acc: 0.6497
Epoch 25/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6697 - acc: 0.6154
491/491 [==============================] - 0s 45us/step - loss: 0.6588 - acc: 0.6517
Epoch 26/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6748 - acc: 0.6154
491/491 [==============================] - 0s 45us/step - loss: 0.6583 - acc: 0.6517
Epoch 27/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6367 - acc: 0.7308
491/491 [==============================] - 0s 45us/step - loss: 0.6579 - acc: 0.6558
Epoch 28/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6236 - acc: 0.7692
491/491 [==============================] - 0s 45us/step - loss: 0.6575 - acc: 0.6538
Epoch 29/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6567 - acc: 0.6538
491/491 [==============================] - 0s 45us/step - loss: 0.6572 - acc: 0.6538
Epoch 30/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6618 - acc: 0.6538
491/491 [==============================] - 0s 45us/step - loss: 0.6568 - acc: 0.6538
Epoch 31/54

 26/491 [>.............................] - ETA: 0s - loss: 0.7192 - acc: 0.4615
491/491 [==============================] - 0s 45us/step - loss: 0.6565 - acc: 0.6538
Epoch 32/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6429 - acc: 0.6923
491/491 [==============================] - 0s 45us/step - loss: 0.6562 - acc: 0.6538
Epoch 33/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6554 - acc: 0.6538
491/491 [==============================] - 0s 45us/step - loss: 0.6559 - acc: 0.6538
Epoch 34/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6814 - acc: 0.5769
491/491 [==============================] - 0s 45us/step - loss: 0.6555 - acc: 0.6538
Epoch 35/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6205 - acc: 0.7692
491/491 [==============================] - 0s 45us/step - loss: 0.6552 - acc: 0.6538
Epoch 36/54

 26/491 [>.............................] - ETA: 0s - loss: 0.7485 - acc: 0.3846
491/491 [==============================] - 0s 45us/step - loss: 0.6550 - acc: 0.6538
Epoch 37/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6273 - acc: 0.7308
491/491 [==============================] - 0s 45us/step - loss: 0.6547 - acc: 0.6538
Epoch 38/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6821 - acc: 0.5769
491/491 [==============================] - 0s 45us/step - loss: 0.6544 - acc: 0.6538
Epoch 39/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6401 - acc: 0.6923
491/491 [==============================] - 0s 45us/step - loss: 0.6542 - acc: 0.6538
Epoch 40/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6397 - acc: 0.6923
491/491 [==============================] - 0s 45us/step - loss: 0.6539 - acc: 0.6538
Epoch 41/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6551 - acc: 0.6538
491/491 [==============================] - 0s 45us/step - loss: 0.6537 - acc: 0.6538
Epoch 42/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6390 - acc: 0.6923
491/491 [==============================] - 0s 45us/step - loss: 0.6535 - acc: 0.6538
Epoch 43/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6244 - acc: 0.7308
491/491 [==============================] - 0s 45us/step - loss: 0.6533 - acc: 0.6538
Epoch 44/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6963 - acc: 0.5385
491/491 [==============================] - 0s 45us/step - loss: 0.6531 - acc: 0.6538
Epoch 45/54

 26/491 [>.............................] - ETA: 0s - loss: 0.5944 - acc: 0.8077
491/491 [==============================] - 0s 45us/step - loss: 0.6529 - acc: 0.6538
Epoch 46/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6231 - acc: 0.7308
491/491 [==============================] - 0s 45us/step - loss: 0.6527 - acc: 0.6538
Epoch 47/54

 26/491 [>.............................] - ETA: 0s - loss: 0.7116 - acc: 0.5000
491/491 [==============================] - 0s 45us/step - loss: 0.6526 - acc: 0.6538
Epoch 48/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6372 - acc: 0.6923
491/491 [==============================] - 0s 45us/step - loss: 0.6524 - acc: 0.6538
Epoch 49/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6670 - acc: 0.6154
491/491 [==============================] - 0s 45us/step - loss: 0.6522 - acc: 0.6538
Epoch 50/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6215 - acc: 0.7308
491/491 [==============================] - 0s 45us/step - loss: 0.6520 - acc: 0.6538
Epoch 51/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6822 - acc: 0.5769
491/491 [==============================] - 0s 45us/step - loss: 0.6519 - acc: 0.6538
Epoch 52/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6822 - acc: 0.5769
491/491 [==============================] - 0s 45us/step - loss: 0.6517 - acc: 0.6538
Epoch 53/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6719 - acc: 0.6154
491/491 [==============================] - 0s 45us/step - loss: 0.6516 - acc: 0.6538
Epoch 54/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6517 - acc: 0.6538
491/491 [==============================] - 0s 45us/step - loss: 0.6514 - acc: 0.6538
[GenericWrapper][DEBUG] Calling runsolver. Command-line:
[GenericWrapper][DEBUG] /home/shalag/.local/lib/python3.5/site-packages/genericWrapper4AC/binaries/runsolver -M 3072 -C 2147483647 -w "/tmp/383598.1.lab_course.q/watcher-304522-0j436_en.log" -o "/tmp/383598.1.lab_course.q/solver-304522-e_75fobx.log" 
[GenericWrapper][DEBUG] Measured wallclock time: 0.009627
[GenericWrapper][DEBUG] Reading runsolver output from /tmp/383598.1.lab_course.q/watcher-304522-0j436_en.log
[GenericWrapper][DEBUG] Measured time by runsolver: 0.009627
Result for ParamILS: SAT, 0.0096, -1, 0.6422764227642277, 491263

2018-05-30 23:08:47:DEBUG:smac.tae.execute_ta_run_old.ExecuteTARunOld:Stderr: 2018-05-30 23:08:45.972306: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
/home/shalag/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.

2018-05-30 23:08:47:DEBUG:smac.tae.execute_ta_run_old.ExecuteTARunOld:Return: Status: <StatusType.SUCCESS: 1>, cost: 0.642276, time: 0.009600, additional: {}
2018-05-30 23:08:47:DEBUG:smac.intensification.intensification.Intensifier:Add run of challenger
2018-05-30 23:08:47:DEBUG:smac.tae.execute_ta_run_old.ExecuteTARunOld:Calling: python3 ./dn_net/KerasWrapper.py --mem-limit 3072 instances/train/instance4.csv 0 99999999999999.0 0 550290313 -activation hard_sigmoid -batch_size 26 -epochs 54 -kernel_initializer lecun_uniform -neurons 1 -optimizer Adagrad
2018-05-30 23:08:51:DEBUG:smac.tae.execute_ta_run_old.ExecuteTARunOld:Stdout: [[  6.    148.     72.    ...   0.627  50.      1.   ]
 [  1.     85.     66.    ...   0.351  31.      0.   ]
 [  8.    183.     64.    ...   0.672  32.      1.   ]
 ...
 [  5.    121.     72.    ...   0.245  30.      0.   ]
 [  1.    126.     60.    ...   0.349  47.      1.   ]
 [  1.     93.     70.    ...   0.315  23.      0.   ]]
Epoch 1/54

 26/492 [>.............................] - ETA: 1s - loss: 0.8735 - acc: 0.5769
492/492 [==============================] - 0s 240us/step - loss: 0.7605 - acc: 0.6280
Epoch 2/54

 26/492 [>.............................] - ETA: 0s - loss: 0.8633 - acc: 0.5385
492/492 [==============================] - 0s 45us/step - loss: 0.7447 - acc: 0.6301
Epoch 3/54

 26/492 [>.............................] - ETA: 0s - loss: 0.7926 - acc: 0.5769
492/492 [==============================] - 0s 45us/step - loss: 0.7354 - acc: 0.6301
Epoch 4/54

 26/492 [>.............................] - ETA: 0s - loss: 0.6519 - acc: 0.6923
492/492 [==============================] - 0s 45us/step - loss: 0.7289 - acc: 0.6301
Epoch 5/54

 26/492 [>.............................] - ETA: 0s - loss: 0.4728 - acc: 0.8077
492/492 [==============================] - 0s 45us/step - loss: 0.7232 - acc: 0.6301
Epoch 6/54

 26/492 [>.............................] - ETA: 0s - loss: 0.8018 - acc: 0.5769
492/492 [==============================] - 0s 45us/step - loss: 0.7186 - acc: 0.6301
Epoch 7/54

 26/492 [>.............................] - ETA: 0s - loss: 0.7594 - acc: 0.6154
492/492 [==============================] - 0s 45us/step - loss: 0.7146 - acc: 0.6301
Epoch 8/54

 26/492 [>.............................] - ETA: 0s - loss: 1.0394 - acc: 0.3846
492/492 [==============================] - 0s 45us/step - loss: 0.7111 - acc: 0.6301
Epoch 9/54

 26/492 [>.............................] - ETA: 0s - loss: 0.7991 - acc: 0.5769
492/492 [==============================] - 0s 45us/step - loss: 0.7080 - acc: 0.6301
Epoch 10/54

 26/492 [>.............................] - ETA: 0s - loss: 0.7053 - acc: 0.6154
492/492 [==============================] - 0s 45us/step - loss: 0.7052 - acc: 0.6301
Epoch 11/54

 26/492 [>.............................] - ETA: 0s - loss: 0.7769 - acc: 0.5769
492/492 [==============================] - 0s 45us/step - loss: 0.7026 - acc: 0.6301
Epoch 12/54

 26/492 [>.............................] - ETA: 0s - loss: 0.9550 - acc: 0.3846
492/492 [==============================] - 0s 45us/step - loss: 0.7002 - acc: 0.6301
Epoch 13/54

 26/492 [>.............................] - ETA: 0s - loss: 0.9075 - acc: 0.4615
492/492 [==============================] - 0s 45us/step - loss: 0.6981 - acc: 0.6301
Epoch 14/54

 26/492 [>.............................] - ETA: 0s - loss: 0.5403 - acc: 0.7692
492/492 [==============================] - 0s 45us/step - loss: 0.6961 - acc: 0.6301
Epoch 15/54

 26/492 [>.............................] - ETA: 0s - loss: 0.7965 - acc: 0.5385
492/492 [==============================] - 0s 45us/step - loss: 0.6943 - acc: 0.6301
Epoch 16/54

 26/492 [>.............................] - ETA: 0s - loss: 0.7935 - acc: 0.5385
492/492 [==============================] - 0s 45us/step - loss: 0.6925 - acc: 0.6301
Epoch 17/54

 26/492 [>.............................] - ETA: 0s - loss: 0.6265 - acc: 0.6538
492/492 [==============================] - 0s 45us/step - loss: 0.6909 - acc: 0.6301
Epoch 18/54

 26/492 [>.............................] - ETA: 0s - loss: 0.6802 - acc: 0.6154
492/492 [==============================] - 0s 45us/step - loss: 0.6894 - acc: 0.6301
Epoch 19/54

 26/492 [>.............................] - ETA: 0s - loss: 0.5836 - acc: 0.7308
492/492 [==============================] - 0s 45us/step - loss: 0.6880 - acc: 0.6301
Epoch 20/54

 26/492 [>.............................] - ETA: 0s - loss: 0.8563 - acc: 0.4615
492/492 [==============================] - 0s 45us/step - loss: 0.6867 - acc: 0.6301
Epoch 21/54

 26/492 [>.............................] - ETA: 0s - loss: 0.5935 - acc: 0.7308
492/492 [==============================] - 0s 45us/step - loss: 0.6854 - acc: 0.6301
Epoch 22/54

 26/492 [>.............................] - ETA: 0s - loss: 0.7983 - acc: 0.5385
492/492 [==============================] - 0s 45us/step - loss: 0.6843 - acc: 0.6301
Epoch 23/54

 26/492 [>.............................] - ETA: 0s - loss: 0.7552 - acc: 0.5769
492/492 [==============================] - 0s 45us/step - loss: 0.6832 - acc: 0.6301
Epoch 24/54

 26/492 [>.............................] - ETA: 0s - loss: 0.6631 - acc: 0.6538
492/492 [==============================] - 0s 45us/step - loss: 0.6821 - acc: 0.6301
Epoch 25/54

 26/492 [>.............................] - ETA: 0s - loss: 0.7910 - acc: 0.5385
492/492 [==============================] - 0s 45us/step - loss: 0.6811 - acc: 0.6301
Epoch 26/54

 26/492 [>.............................] - ETA: 0s - loss: 0.7432 - acc: 0.5385
492/492 [==============================] - 0s 45us/step - loss: 0.6802 - acc: 0.6301
Epoch 27/54

 26/492 [>.............................] - ETA: 0s - loss: 0.7122 - acc: 0.5769
492/492 [==============================] - 0s 45us/step - loss: 0.6793 - acc: 0.6301
Epoch 28/54

 26/492 [>.............................] - ETA: 0s - loss: 0.7846 - acc: 0.5385
492/492 [==============================] - 0s 45us/step - loss: 0.6784 - acc: 0.6301
Epoch 29/54

 26/492 [>.............................] - ETA: 0s - loss: 0.7356 - acc: 0.5769
492/492 [==============================] - 0s 45us/step - loss: 0.6776 - acc: 0.6301
Epoch 30/54

 26/492 [>.............................] - ETA: 0s - loss: 0.6204 - acc: 0.6923
492/492 [==============================] - 0s 45us/step - loss: 0.6769 - acc: 0.6301
Epoch 31/54

 26/492 [>.............................] - ETA: 0s - loss: 0.7336 - acc: 0.5769
492/492 [==============================] - 0s 45us/step - loss: 0.6761 - acc: 0.6301
Epoch 32/54

 26/492 [>.............................] - ETA: 0s - loss: 0.6198 - acc: 0.6923
492/492 [==============================] - 0s 45us/step - loss: 0.6755 - acc: 0.6301
Epoch 33/54

 26/492 [>.............................] - ETA: 0s - loss: 0.5826 - acc: 0.7308
492/492 [==============================] - 0s 45us/step - loss: 0.6748 - acc: 0.6301
Epoch 34/54

 26/492 [>.............................] - ETA: 0s - loss: 0.7587 - acc: 0.5385
492/492 [==============================] - 0s 45us/step - loss: 0.6742 - acc: 0.6301
Epoch 35/54

 26/492 [>.............................] - ETA: 0s - loss: 0.6697 - acc: 0.6154
492/492 [==============================] - 0s 45us/step - loss: 0.6736 - acc: 0.6301
Epoch 36/54

 26/492 [>.............................] - ETA: 0s - loss: 0.6628 - acc: 0.6538
492/492 [==============================] - 0s 45us/step - loss: 0.6730 - acc: 0.6301
Epoch 37/54

 26/492 [>.............................] - ETA: 0s - loss: 0.7116 - acc: 0.5769
492/492 [==============================] - 0s 45us/step - loss: 0.6725 - acc: 0.6301
Epoch 38/54

 26/492 [>.............................] - ETA: 0s - loss: 0.6979 - acc: 0.6154
492/492 [==============================] - 0s 45us/step - loss: 0.6719 - acc: 0.6301
Epoch 39/54

 26/492 [>.............................] - ETA: 0s - loss: 0.8233 - acc: 0.4615
492/492 [==============================] - 0s 45us/step - loss: 0.6714 - acc: 0.6301
Epoch 40/54

 26/492 [>.............................] - ETA: 0s - loss: 0.6183 - acc: 0.6923
492/492 [==============================] - 0s 45us/step - loss: 0.6710 - acc: 0.6301
Epoch 41/54

 26/492 [>.............................] - ETA: 0s - loss: 0.8131 - acc: 0.4615
492/492 [==============================] - 0s 45us/step - loss: 0.6705 - acc: 0.6301
Epoch 42/54

 26/492 [>.............................] - ETA: 0s - loss: 0.6525 - acc: 0.6538
492/492 [==============================] - 0s 45us/step - loss: 0.6701 - acc: 0.6301
Epoch 43/54

 26/492 [>.............................] - ETA: 0s - loss: 0.7206 - acc: 0.5769
492/492 [==============================] - 0s 45us/step - loss: 0.6696 - acc: 0.6301
Epoch 44/54

 26/492 [>.............................] - ETA: 0s - loss: 0.6858 - acc: 0.6154
492/492 [==============================] - 0s 45us/step - loss: 0.6692 - acc: 0.6301
Epoch 45/54

 26/492 [>.............................] - ETA: 0s - loss: 0.7741 - acc: 0.5000
492/492 [==============================] - 0s 45us/step - loss: 0.6689 - acc: 0.6301
Epoch 46/54

 26/492 [>.............................] - ETA: 0s - loss: 0.5571 - acc: 0.7692
492/492 [==============================] - 0s 45us/step - loss: 0.6685 - acc: 0.6301
Epoch 47/54

 26/492 [>.............................] - ETA: 0s - loss: 0.7722 - acc: 0.5000
492/492 [==============================] - 0s 45us/step - loss: 0.6681 - acc: 0.6301
Epoch 48/54

 26/492 [>.............................] - ETA: 0s - loss: 0.6901 - acc: 0.6154
492/492 [==============================] - 0s 45us/step - loss: 0.6678 - acc: 0.6301
Epoch 49/54

 26/492 [>.............................] - ETA: 0s - loss: 0.6833 - acc: 0.6154
492/492 [==============================] - 0s 45us/step - loss: 0.6675 - acc: 0.6301
Epoch 50/54

 26/492 [>.............................] - ETA: 0s - loss: 0.6889 - acc: 0.6154
492/492 [==============================] - 0s 45us/step - loss: 0.6672 - acc: 0.6301
Epoch 51/54

 26/492 [>.............................] - ETA: 0s - loss: 0.7422 - acc: 0.5385
492/492 [==============================] - 0s 45us/step - loss: 0.6669 - acc: 0.6301
Epoch 52/54

 26/492 [>.............................] - ETA: 0s - loss: 0.6496 - acc: 0.6538
492/492 [==============================] - 0s 45us/step - loss: 0.6666 - acc: 0.6301
Epoch 53/54

 26/492 [>.............................] - ETA: 0s - loss: 0.5315 - acc: 0.7692
492/492 [==============================] - 0s 45us/step - loss: 0.6663 - acc: 0.6301
Epoch 54/54

 26/492 [>.............................] - ETA: 0s - loss: 0.5862 - acc: 0.7308
492/492 [==============================] - 0s 45us/step - loss: 0.6661 - acc: 0.6301
[GenericWrapper][DEBUG] Calling runsolver. Command-line:
[GenericWrapper][DEBUG] /home/shalag/.local/lib/python3.5/site-packages/genericWrapper4AC/binaries/runsolver -M 3072 -C 2147483647 -w "/tmp/383598.1.lab_course.q/watcher-131817-lgm8vviq.log" -o "/tmp/383598.1.lab_course.q/solver-131817-7bvd_kw3.log" 
[GenericWrapper][DEBUG] Measured wallclock time: 0.009383
[GenericWrapper][DEBUG] Reading runsolver output from /tmp/383598.1.lab_course.q/watcher-131817-lgm8vviq.log
[GenericWrapper][DEBUG] Measured time by runsolver: 0.009383
Result for ParamILS: SAT, 0.0094, -1, 0.6178861788617886, 550290313

2018-05-30 23:08:51:DEBUG:smac.tae.execute_ta_run_old.ExecuteTARunOld:Stderr: 2018-05-30 23:08:49.688780: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
/home/shalag/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.

2018-05-30 23:08:51:DEBUG:smac.tae.execute_ta_run_old.ExecuteTARunOld:Return: Status: <StatusType.SUCCESS: 1>, cost: 0.617886, time: 0.009400, additional: {}
2018-05-30 23:08:51:INFO:smac.intensification.intensification.Intensifier:Challenger (0.6301) is better than incumbent (0.6341) on 2 runs.
2018-05-30 23:08:51:INFO:smac.intensification.intensification.Intensifier:Changes in incumbent:
2018-05-30 23:08:51:INFO:smac.intensification.intensification.Intensifier:  activation : 'relu' -> 'hard_sigmoid'
2018-05-30 23:08:51:INFO:smac.intensification.intensification.Intensifier:  batch_size : 10 -> 26
2018-05-30 23:08:51:INFO:smac.intensification.intensification.Intensifier:  epochs : 10 -> 54
2018-05-30 23:08:51:INFO:smac.intensification.intensification.Intensifier:  kernel_initializer : 'uniform' -> 'lecun_uniform'
2018-05-30 23:08:51:DEBUG:smac.intensification.intensification.Intensifier:  neurons remains unchanged: 1
2018-05-30 23:08:51:INFO:smac.intensification.intensification.Intensifier:  optimizer : 'Adam' -> 'Adagrad'
2018-05-30 23:08:51:DEBUG:smac.intensification.intensification.Intensifier:Intensify on Configuration:
  activation, Value: 'tanh'
  batch_size, Value: 35
  epochs, Value: 71
  kernel_initializer, Value: 'he_uniform'
  neurons, Value: 2
  optimizer, Value: 'Adam'

2018-05-30 23:08:51:DEBUG:smac.intensification.intensification.Intensifier:Configuration origin: Random Search
2018-05-30 23:08:51:DEBUG:smac.intensification.intensification.Intensifier:Add run of incumbent
2018-05-30 23:08:51:DEBUG:smac.tae.execute_ta_run_old.ExecuteTARunOld:Calling: python3 ./dn_net/KerasWrapper.py --mem-limit 3072 instances/train/instance3.csv 0 99999999999999.0 0 630311759 -activation hard_sigmoid -batch_size 26 -epochs 54 -kernel_initializer lecun_uniform -neurons 1 -optimizer Adagrad
2018-05-30 23:08:54:DEBUG:smac.tae.execute_ta_run_old.ExecuteTARunOld:Stdout: [[  6.    148.     72.    ...   0.627  50.      1.   ]
 [  1.     85.     66.    ...   0.351  31.      0.   ]
 [  8.    183.     64.    ...   0.672  32.      1.   ]
 ...
 [  5.    121.     72.    ...   0.245  30.      0.   ]
 [  1.    126.     60.    ...   0.349  47.      1.   ]
 [  1.     93.     70.    ...   0.315  23.      0.   ]]
Epoch 1/54

 26/491 [>.............................] - ETA: 1s - loss: 0.6953 - acc: 0.3846
491/491 [==============================] - 0s 242us/step - loss: 0.7157 - acc: 0.5031
Epoch 2/54

 26/491 [>.............................] - ETA: 0s - loss: 0.7009 - acc: 0.5769
491/491 [==============================] - 0s 45us/step - loss: 0.6922 - acc: 0.5988
Epoch 3/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6995 - acc: 0.5769
491/491 [==============================] - 0s 45us/step - loss: 0.6850 - acc: 0.6273
Epoch 4/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6713 - acc: 0.7308
491/491 [==============================] - 0s 45us/step - loss: 0.6808 - acc: 0.6354
Epoch 5/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6816 - acc: 0.6154
491/491 [==============================] - 0s 45us/step - loss: 0.6780 - acc: 0.6415
Epoch 6/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6917 - acc: 0.5385
491/491 [==============================] - 0s 45us/step - loss: 0.6758 - acc: 0.6477
Epoch 7/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6623 - acc: 0.7308
491/491 [==============================] - 0s 45us/step - loss: 0.6739 - acc: 0.6477
Epoch 8/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6869 - acc: 0.5385
491/491 [==============================] - 0s 45us/step - loss: 0.6722 - acc: 0.6477
Epoch 9/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6499 - acc: 0.7692
491/491 [==============================] - 0s 45us/step - loss: 0.6707 - acc: 0.6497
Epoch 10/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6834 - acc: 0.5769
491/491 [==============================] - 0s 45us/step - loss: 0.6695 - acc: 0.6517
Epoch 11/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6685 - acc: 0.6538
491/491 [==============================] - 0s 45us/step - loss: 0.6683 - acc: 0.6517
Epoch 12/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6599 - acc: 0.6923
491/491 [==============================] - 0s 45us/step - loss: 0.6672 - acc: 0.6538
Epoch 13/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6603 - acc: 0.6923
491/491 [==============================] - 0s 45us/step - loss: 0.6662 - acc: 0.6538
Epoch 14/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6302 - acc: 0.8077
491/491 [==============================] - 0s 45us/step - loss: 0.6652 - acc: 0.6538
Epoch 15/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6563 - acc: 0.6923
491/491 [==============================] - 0s 45us/step - loss: 0.6643 - acc: 0.6538
Epoch 16/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6433 - acc: 0.7308
491/491 [==============================] - 0s 45us/step - loss: 0.6635 - acc: 0.6538
Epoch 17/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6601 - acc: 0.6538
491/491 [==============================] - 0s 45us/step - loss: 0.6627 - acc: 0.6538
Epoch 18/54

 26/491 [>.............................] - ETA: 0s - loss: 0.7064 - acc: 0.4615
491/491 [==============================] - 0s 45us/step - loss: 0.6619 - acc: 0.6558
Epoch 19/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6331 - acc: 0.7692
491/491 [==============================] - 0s 45us/step - loss: 0.6612 - acc: 0.6558
Epoch 20/54

 26/491 [>.............................] - ETA: 0s - loss: 0.7051 - acc: 0.5000
491/491 [==============================] - 0s 45us/step - loss: 0.6606 - acc: 0.6578
Epoch 21/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6509 - acc: 0.6923
491/491 [==============================] - 0s 45us/step - loss: 0.6600 - acc: 0.6578
Epoch 22/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6606 - acc: 0.6538
491/491 [==============================] - 0s 45us/step - loss: 0.6594 - acc: 0.6578
Epoch 23/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6708 - acc: 0.6154
491/491 [==============================] - 0s 45us/step - loss: 0.6588 - acc: 0.6578
Epoch 24/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6597 - acc: 0.6538
491/491 [==============================] - 0s 45us/step - loss: 0.6583 - acc: 0.6578
Epoch 25/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6592 - acc: 0.6538
491/491 [==============================] - 0s 45us/step - loss: 0.6578 - acc: 0.6578
Epoch 26/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6412 - acc: 0.7308
491/491 [==============================] - 0s 45us/step - loss: 0.6573 - acc: 0.6578
Epoch 27/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6469 - acc: 0.6923
491/491 [==============================] - 0s 45us/step - loss: 0.6569 - acc: 0.6578
Epoch 28/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6463 - acc: 0.6923
491/491 [==============================] - 0s 45us/step - loss: 0.6564 - acc: 0.6599
Epoch 29/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6931 - acc: 0.5385
491/491 [==============================] - 0s 45us/step - loss: 0.6560 - acc: 0.6599
Epoch 30/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6746 - acc: 0.6154
491/491 [==============================] - 0s 45us/step - loss: 0.6556 - acc: 0.6599
Epoch 31/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6952 - acc: 0.5385
491/491 [==============================] - 0s 45us/step - loss: 0.6552 - acc: 0.6599
Epoch 32/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6195 - acc: 0.7692
491/491 [==============================] - 0s 45us/step - loss: 0.6548 - acc: 0.6599
Epoch 33/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6938 - acc: 0.5385
491/491 [==============================] - 0s 45us/step - loss: 0.6545 - acc: 0.6599
Epoch 34/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6686 - acc: 0.6154
491/491 [==============================] - 0s 45us/step - loss: 0.6541 - acc: 0.6599
Epoch 35/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6299 - acc: 0.7308
491/491 [==============================] - 0s 45us/step - loss: 0.6538 - acc: 0.6599
Epoch 36/54

 26/491 [>.............................] - ETA: 0s - loss: 0.7211 - acc: 0.4615
491/491 [==============================] - 0s 45us/step - loss: 0.6534 - acc: 0.6599
Epoch 37/54

 26/491 [>.............................] - ETA: 0s - loss: 0.5893 - acc: 0.8462
491/491 [==============================] - 0s 45us/step - loss: 0.6531 - acc: 0.6599
Epoch 38/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6681 - acc: 0.6154
491/491 [==============================] - 0s 45us/step - loss: 0.6528 - acc: 0.6599
Epoch 39/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6411 - acc: 0.6923
491/491 [==============================] - 0s 45us/step - loss: 0.6526 - acc: 0.6599
Epoch 40/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6543 - acc: 0.6538
491/491 [==============================] - 0s 45us/step - loss: 0.6523 - acc: 0.6599
Epoch 41/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6678 - acc: 0.6154
491/491 [==============================] - 0s 45us/step - loss: 0.6520 - acc: 0.6599
Epoch 42/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6677 - acc: 0.6154
491/491 [==============================] - 0s 45us/step - loss: 0.6518 - acc: 0.6599
Epoch 43/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6396 - acc: 0.6923
491/491 [==============================] - 0s 45us/step - loss: 0.6515 - acc: 0.6599
Epoch 44/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6675 - acc: 0.6154
491/491 [==============================] - 0s 45us/step - loss: 0.6513 - acc: 0.6599
Epoch 45/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6104 - acc: 0.7692
491/491 [==============================] - 0s 45us/step - loss: 0.6511 - acc: 0.6599
Epoch 46/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6530 - acc: 0.6538
491/491 [==============================] - 0s 45us/step - loss: 0.6509 - acc: 0.6599
Epoch 47/54

 26/491 [>.............................] - ETA: 0s - loss: 0.7689 - acc: 0.3462
491/491 [==============================] - 0s 45us/step - loss: 0.6506 - acc: 0.6599
Epoch 48/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6234 - acc: 0.7308
491/491 [==============================] - 0s 45us/step - loss: 0.6504 - acc: 0.6599
Epoch 49/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6377 - acc: 0.6923
491/491 [==============================] - 0s 45us/step - loss: 0.6502 - acc: 0.6599
Epoch 50/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6572 - acc: 0.6538
491/491 [==============================] - 0s 45us/step - loss: 0.6500 - acc: 0.6599
Epoch 51/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6521 - acc: 0.6538
491/491 [==============================] - 0s 45us/step - loss: 0.6498 - acc: 0.6599
Epoch 52/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6368 - acc: 0.6923
491/491 [==============================] - 0s 45us/step - loss: 0.6497 - acc: 0.6599
Epoch 53/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6821 - acc: 0.5769
491/491 [==============================] - 0s 45us/step - loss: 0.6495 - acc: 0.6599
Epoch 54/54

 26/491 [>.............................] - ETA: 0s - loss: 0.6516 - acc: 0.6538
491/491 [==============================] - 0s 45us/step - loss: 0.6493 - acc: 0.6599
[GenericWrapper][DEBUG] Calling runsolver. Command-line:
[GenericWrapper][DEBUG] /home/shalag/.local/lib/python3.5/site-packages/genericWrapper4AC/binaries/runsolver -M 3072 -C 2147483647 -w "/tmp/383598.1.lab_course.q/watcher-600974-5_61xvlw.log" -o "/tmp/383598.1.lab_course.q/solver-600974-05wt8tlc.log" 
[GenericWrapper][DEBUG] Measured wallclock time: 0.009454
[GenericWrapper][DEBUG] Reading runsolver output from /tmp/383598.1.lab_course.q/watcher-600974-5_61xvlw.log
[GenericWrapper][DEBUG] Measured time by runsolver: 0.009454
Result for ParamILS: SAT, 0.0095, -1, 0.6422764227642277, 630311759

2018-05-30 23:08:54:DEBUG:smac.tae.execute_ta_run_old.ExecuteTARunOld:Stderr: 2018-05-30 23:08:53.406536: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
/home/shalag/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.

2018-05-30 23:08:54:DEBUG:smac.tae.execute_ta_run_old.ExecuteTARunOld:Return: Status: <StatusType.SUCCESS: 1>, cost: 0.642276, time: 0.009500, additional: {}
2018-05-30 23:08:54:DEBUG:smac.intensification.intensification.Intensifier:Add run of challenger
2018-05-30 23:08:54:DEBUG:smac.tae.execute_ta_run_old.ExecuteTARunOld:Calling: python3 ./dn_net/KerasWrapper.py --mem-limit 3072 instances/train/instance3.csv 0 99999999999999.0 0 630311759 -activation tanh -batch_size 35 -epochs 71 -kernel_initializer he_uniform -neurons 2 -optimizer Adam
2018-05-30 23:08:58:DEBUG:smac.tae.execute_ta_run_old.ExecuteTARunOld:Stdout: [[  6.    148.     72.    ...   0.627  50.      1.   ]
 [  1.     85.     66.    ...   0.351  31.      0.   ]
 [  8.    183.     64.    ...   0.672  32.      1.   ]
 ...
 [  5.    121.     72.    ...   0.245  30.      0.   ]
 [  1.    126.     60.    ...   0.349  47.      1.   ]
 [  1.     93.     70.    ...   0.315  23.      0.   ]]
Epoch 1/71

 35/491 [=>............................] - ETA: 1s - loss: 0.8471 - acc: 0.5429
491/491 [==============================] - 0s 274us/step - loss: 0.7790 - acc: 0.6191
Epoch 2/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6915 - acc: 0.6857
491/491 [==============================] - 0s 37us/step - loss: 0.7733 - acc: 0.6212
Epoch 3/71

 35/491 [=>............................] - ETA: 0s - loss: 0.7816 - acc: 0.6286
491/491 [==============================] - 0s 37us/step - loss: 0.7721 - acc: 0.6151
Epoch 4/71

 35/491 [=>............................] - ETA: 0s - loss: 0.5394 - acc: 0.8000
491/491 [==============================] - 0s 37us/step - loss: 0.7636 - acc: 0.6212
Epoch 5/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6983 - acc: 0.6571
491/491 [==============================] - 0s 37us/step - loss: 0.7555 - acc: 0.6212
Epoch 6/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6553 - acc: 0.6857
491/491 [==============================] - 0s 37us/step - loss: 0.7539 - acc: 0.6171
Epoch 7/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6882 - acc: 0.6857
491/491 [==============================] - 0s 37us/step - loss: 0.7502 - acc: 0.6212
Epoch 8/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6867 - acc: 0.6571
491/491 [==============================] - 0s 37us/step - loss: 0.7457 - acc: 0.6191
Epoch 9/71

 35/491 [=>............................] - ETA: 0s - loss: 0.7746 - acc: 0.6000
491/491 [==============================] - 0s 37us/step - loss: 0.7409 - acc: 0.6191
Epoch 10/71

 35/491 [=>............................] - ETA: 0s - loss: 0.8799 - acc: 0.4857
491/491 [==============================] - 0s 37us/step - loss: 0.7360 - acc: 0.6212
Epoch 11/71

 35/491 [=>............................] - ETA: 0s - loss: 0.8618 - acc: 0.5429
491/491 [==============================] - 0s 37us/step - loss: 0.7325 - acc: 0.6232
Epoch 12/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6250 - acc: 0.7143
491/491 [==============================] - 0s 37us/step - loss: 0.7303 - acc: 0.6232
Epoch 13/71

 35/491 [=>............................] - ETA: 0s - loss: 0.7118 - acc: 0.6286
491/491 [==============================] - 0s 37us/step - loss: 0.7268 - acc: 0.6232
Epoch 14/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6279 - acc: 0.7143
491/491 [==============================] - 0s 37us/step - loss: 0.7224 - acc: 0.6273
Epoch 15/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6477 - acc: 0.6857
491/491 [==============================] - 0s 37us/step - loss: 0.7193 - acc: 0.6273
Epoch 16/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6287 - acc: 0.7143
491/491 [==============================] - 0s 37us/step - loss: 0.7154 - acc: 0.6334
Epoch 17/71

 35/491 [=>............................] - ETA: 0s - loss: 0.7475 - acc: 0.6000
491/491 [==============================] - 0s 37us/step - loss: 0.7138 - acc: 0.6334
Epoch 18/71

 35/491 [=>............................] - ETA: 0s - loss: 0.7708 - acc: 0.5714
491/491 [==============================] - 0s 37us/step - loss: 0.7124 - acc: 0.6334
Epoch 19/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6120 - acc: 0.7143
491/491 [==============================] - 0s 37us/step - loss: 0.7106 - acc: 0.6334
Epoch 20/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6652 - acc: 0.6571
491/491 [==============================] - 0s 37us/step - loss: 0.7088 - acc: 0.6334
Epoch 21/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6074 - acc: 0.7143
491/491 [==============================] - 0s 37us/step - loss: 0.7074 - acc: 0.6334
Epoch 22/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6069 - acc: 0.7143
491/491 [==============================] - 0s 37us/step - loss: 0.7056 - acc: 0.6334
Epoch 23/71

 35/491 [=>............................] - ETA: 0s - loss: 0.7398 - acc: 0.6000
491/491 [==============================] - 0s 37us/step - loss: 0.7041 - acc: 0.6334
Epoch 24/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6086 - acc: 0.7143
491/491 [==============================] - 0s 37us/step - loss: 0.7028 - acc: 0.6334
Epoch 25/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6592 - acc: 0.6571
491/491 [==============================] - 0s 37us/step - loss: 0.7015 - acc: 0.6334
Epoch 26/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6031 - acc: 0.7143
491/491 [==============================] - 0s 37us/step - loss: 0.7003 - acc: 0.6334
Epoch 27/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6081 - acc: 0.7143
491/491 [==============================] - 0s 37us/step - loss: 0.6987 - acc: 0.6334
Epoch 28/71

 35/491 [=>............................] - ETA: 0s - loss: 0.7752 - acc: 0.5714
491/491 [==============================] - 0s 37us/step - loss: 0.6972 - acc: 0.6334
Epoch 29/71

 35/491 [=>............................] - ETA: 0s - loss: 0.7549 - acc: 0.5714
491/491 [==============================] - 0s 37us/step - loss: 0.6959 - acc: 0.6334
Epoch 30/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6092 - acc: 0.7143
491/491 [==============================] - 0s 37us/step - loss: 0.6946 - acc: 0.6334
Epoch 31/71

 35/491 [=>............................] - ETA: 0s - loss: 0.7592 - acc: 0.5714
491/491 [==============================] - 0s 37us/step - loss: 0.6937 - acc: 0.6334
Epoch 32/71

 35/491 [=>............................] - ETA: 0s - loss: 0.7964 - acc: 0.5429
491/491 [==============================] - 0s 37us/step - loss: 0.6925 - acc: 0.6334
Epoch 33/71

 35/491 [=>............................] - ETA: 0s - loss: 0.7649 - acc: 0.5714
491/491 [==============================] - 0s 38us/step - loss: 0.6914 - acc: 0.6334
Epoch 34/71

 35/491 [=>............................] - ETA: 0s - loss: 0.7875 - acc: 0.5429
491/491 [==============================] - 0s 38us/step - loss: 0.6902 - acc: 0.6334
Epoch 35/71

 35/491 [=>............................] - ETA: 0s - loss: 0.7253 - acc: 0.6000
491/491 [==============================] - 0s 38us/step - loss: 0.6893 - acc: 0.6334
Epoch 36/71

 35/491 [=>............................] - ETA: 0s - loss: 0.7580 - acc: 0.5714
491/491 [==============================] - 0s 38us/step - loss: 0.6882 - acc: 0.6334
Epoch 37/71

 35/491 [=>............................] - ETA: 0s - loss: 0.5714 - acc: 0.7429
491/491 [==============================] - 0s 38us/step - loss: 0.6872 - acc: 0.6334
Epoch 38/71

 35/491 [=>............................] - ETA: 0s - loss: 0.7505 - acc: 0.5714
491/491 [==============================] - 0s 38us/step - loss: 0.6861 - acc: 0.6334
Epoch 39/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6300 - acc: 0.6857
491/491 [==============================] - 0s 38us/step - loss: 0.6847 - acc: 0.6334
Epoch 40/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6279 - acc: 0.6857
491/491 [==============================] - 0s 38us/step - loss: 0.6833 - acc: 0.6334
Epoch 41/71

 35/491 [=>............................] - ETA: 0s - loss: 0.8628 - acc: 0.4571
491/491 [==============================] - 0s 38us/step - loss: 0.6824 - acc: 0.6334
Epoch 42/71

 35/491 [=>............................] - ETA: 0s - loss: 0.7465 - acc: 0.5714
491/491 [==============================] - 0s 38us/step - loss: 0.6814 - acc: 0.6334
Epoch 43/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6868 - acc: 0.6286
491/491 [==============================] - 0s 38us/step - loss: 0.6806 - acc: 0.6334
Epoch 44/71

 35/491 [=>............................] - ETA: 0s - loss: 0.5976 - acc: 0.7143
491/491 [==============================] - 0s 38us/step - loss: 0.6795 - acc: 0.6334
Epoch 45/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6753 - acc: 0.6286
491/491 [==============================] - 0s 38us/step - loss: 0.6784 - acc: 0.6334
Epoch 46/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6807 - acc: 0.6286
491/491 [==============================] - 0s 38us/step - loss: 0.6771 - acc: 0.6334
Epoch 47/71

 35/491 [=>............................] - ETA: 0s - loss: 0.8639 - acc: 0.4571
491/491 [==============================] - 0s 38us/step - loss: 0.6761 - acc: 0.6334
Epoch 48/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6469 - acc: 0.6571
491/491 [==============================] - 0s 38us/step - loss: 0.6751 - acc: 0.6334
Epoch 49/71

 35/491 [=>............................] - ETA: 0s - loss: 0.7300 - acc: 0.5714
491/491 [==============================] - 0s 38us/step - loss: 0.6743 - acc: 0.6334
Epoch 50/71

 35/491 [=>............................] - ETA: 0s - loss: 0.7094 - acc: 0.6000
491/491 [==============================] - 0s 38us/step - loss: 0.6728 - acc: 0.6334
Epoch 51/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6770 - acc: 0.6286
491/491 [==============================] - 0s 38us/step - loss: 0.6720 - acc: 0.6334
Epoch 52/71

 35/491 [=>............................] - ETA: 0s - loss: 0.7351 - acc: 0.5714
491/491 [==============================] - 0s 38us/step - loss: 0.6714 - acc: 0.6334
Epoch 53/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6626 - acc: 0.6571
491/491 [==============================] - 0s 38us/step - loss: 0.6705 - acc: 0.6334
Epoch 54/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6482 - acc: 0.6571
491/491 [==============================] - 0s 38us/step - loss: 0.6698 - acc: 0.6334
Epoch 55/71

 35/491 [=>............................] - ETA: 0s - loss: 0.7158 - acc: 0.6000
491/491 [==============================] - 0s 38us/step - loss: 0.6687 - acc: 0.6334
Epoch 56/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6926 - acc: 0.6000
491/491 [==============================] - 0s 38us/step - loss: 0.6679 - acc: 0.6334
Epoch 57/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6322 - acc: 0.6571
491/491 [==============================] - 0s 38us/step - loss: 0.6671 - acc: 0.6334
Epoch 58/71

 35/491 [=>............................] - ETA: 0s - loss: 0.7201 - acc: 0.5714
491/491 [==============================] - 0s 38us/step - loss: 0.6663 - acc: 0.6334
Epoch 59/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6397 - acc: 0.6571
491/491 [==============================] - 0s 38us/step - loss: 0.6653 - acc: 0.6334
Epoch 60/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6514 - acc: 0.6571
491/491 [==============================] - 0s 38us/step - loss: 0.6668 - acc: 0.6253
Epoch 61/71

 35/491 [=>............................] - ETA: 0s - loss: 0.7269 - acc: 0.5429
491/491 [==============================] - 0s 38us/step - loss: 0.6719 - acc: 0.6212
Epoch 62/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6719 - acc: 0.6000
491/491 [==============================] - 0s 38us/step - loss: 0.6712 - acc: 0.6171
Epoch 63/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6821 - acc: 0.6286
491/491 [==============================] - 0s 38us/step - loss: 0.6679 - acc: 0.6171
Epoch 64/71

 35/491 [=>............................] - ETA: 0s - loss: 0.7194 - acc: 0.5714
491/491 [==============================] - 0s 38us/step - loss: 0.6655 - acc: 0.6212
Epoch 65/71

 35/491 [=>............................] - ETA: 0s - loss: 0.7178 - acc: 0.5714
491/491 [==============================] - 0s 38us/step - loss: 0.6642 - acc: 0.6212
Epoch 66/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6407 - acc: 0.6286
491/491 [==============================] - 0s 38us/step - loss: 0.6633 - acc: 0.6212
Epoch 67/71

 35/491 [=>............................] - ETA: 0s - loss: 0.7591 - acc: 0.5429
491/491 [==============================] - 0s 38us/step - loss: 0.6623 - acc: 0.6212
Epoch 68/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6781 - acc: 0.6286
491/491 [==============================] - 0s 38us/step - loss: 0.6611 - acc: 0.6232
Epoch 69/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6556 - acc: 0.6286
491/491 [==============================] - 0s 38us/step - loss: 0.6600 - acc: 0.6232
Epoch 70/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6092 - acc: 0.6571
491/491 [==============================] - 0s 38us/step - loss: 0.6620 - acc: 0.6191
Epoch 71/71

 35/491 [=>............................] - ETA: 0s - loss: 0.6220 - acc: 0.6571
491/491 [==============================] - 0s 38us/step - loss: 0.6620 - acc: 0.6191
[GenericWrapper][DEBUG] Calling runsolver. Command-line:
[GenericWrapper][DEBUG] /home/shalag/.local/lib/python3.5/site-packages/genericWrapper4AC/binaries/runsolver -M 3072 -C 2147483647 -w "/tmp/383598.1.lab_course.q/watcher-829246-esnxtbxo.log" -o "/tmp/383598.1.lab_course.q/solver-829246-mep6dneh.log" 
[GenericWrapper][DEBUG] Measured wallclock time: 0.010256
[GenericWrapper][DEBUG] Reading runsolver output from /tmp/383598.1.lab_course.q/watcher-829246-esnxtbxo.log
[GenericWrapper][DEBUG] Measured time by runsolver: 0.010256
Result for ParamILS: SAT, 0.0103, -1, 0.6260162601626016, 630311759

2018-05-30 23:08:58:DEBUG:smac.tae.execute_ta_run_old.ExecuteTARunOld:Stderr: 2018-05-30 23:08:57.149196: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
/home/shalag/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.

2018-05-30 23:08:58:DEBUG:smac.tae.execute_ta_run_old.ExecuteTARunOld:Return: Status: <StatusType.SUCCESS: 1>, cost: 0.626016, time: 0.010300, additional: {}
2018-05-30 23:08:58:DEBUG:smac.intensification.intensification.Intensifier:Add run of challenger
2018-05-30 23:08:58:DEBUG:smac.tae.execute_ta_run_old.ExecuteTARunOld:Calling: python3 ./dn_net/KerasWrapper.py --mem-limit 3072 instances/train/instance4.csv 0 99999999999999.0 0 550290313 -activation tanh -batch_size 35 -epochs 71 -kernel_initializer he_uniform -neurons 2 -optimizer Adam
